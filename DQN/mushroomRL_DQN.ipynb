{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f35f50ea",
   "metadata": {},
   "source": [
    "# Solving Atari with DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd6b611b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pybullet build time: Dec  1 2021 18:33:04\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from mushroom_rl.algorithms.value import DQN\n",
    "from mushroom_rl.approximators.parametric import TorchApproximator\n",
    "from mushroom_rl.core import Core\n",
    "from mushroom_rl.environments import Atari\n",
    "from mushroom_rl.policy import EpsGreedy\n",
    "from mushroom_rl.utils.dataset import compute_metrics\n",
    "from mushroom_rl.utils.parameters import LinearParameter, Parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3445a3",
   "metadata": {},
   "source": [
    "Here, we defined a generic Q network, which can return the whole actions per state or just one action (filling the `action` parameter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3bbd65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    n_features = 512\n",
    "\n",
    "    def __init__(self, input_shape, output_shape, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        n_input = input_shape[0]\n",
    "        n_output = output_shape[0]\n",
    "\n",
    "        self._h1 = nn.Conv2d(n_input, 32, kernel_size=8, stride=4)\n",
    "        self._h2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self._h3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        self._h4 = nn.Linear(3136, self.n_features)\n",
    "        self._h5 = nn.Linear(self.n_features, n_output)\n",
    "        \n",
    "        # init each weights with Glorot initialization\n",
    "        nn.init.xavier_uniform_(self._h1.weight,\n",
    "                                gain=nn.init.calculate_gain('relu'))\n",
    "        nn.init.xavier_uniform_(self._h2.weight,\n",
    "                                gain=nn.init.calculate_gain('relu'))\n",
    "        nn.init.xavier_uniform_(self._h3.weight,\n",
    "                                gain=nn.init.calculate_gain('relu'))\n",
    "        nn.init.xavier_uniform_(self._h4.weight,\n",
    "                                gain=nn.init.calculate_gain('relu'))\n",
    "        nn.init.xavier_uniform_(self._h5.weight,\n",
    "                                gain=nn.init.calculate_gain('linear'))\n",
    "\n",
    "    def forward(self, state, action=None):\n",
    "        h = F.relu(self._h1(state.float() / 255.))\n",
    "        h = F.relu(self._h2(h))\n",
    "        h = F.relu(self._h3(h))\n",
    "        h = F.relu(self._h4(h.view(-1, 3136)))\n",
    "        q = self._h5(h)\n",
    "\n",
    "        if action is None:\n",
    "            return q\n",
    "        else:\n",
    "            q_acted = torch.squeeze(q.gather(1, action.long()))\n",
    "\n",
    "            return q_acted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f9bff31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_epoch(epoch):\n",
    "    print('################################################################')\n",
    "    print('Epoch: ', epoch)\n",
    "    print('----------------------------------------------------------------')\n",
    "\n",
    "\n",
    "def get_stats(dataset):\n",
    "    \"\"\"\n",
    "    dataset: it is produced when evaluate the agent on the mdp. Or in other\n",
    "             words, when it moved the agent in the environment \n",
    "             using its policy.\n",
    "    \"\"\"\n",
    "    score = compute_metrics(dataset)\n",
    "    print(('min_reward: %f, max_reward: %f, mean_reward: %f,'\n",
    "          ' games_completed: %d' % score))\n",
    "\n",
    "    return score\n",
    "\n",
    "# list to store the scores\n",
    "scores = list()\n",
    "\n",
    "# optimizaer to the approximator\n",
    "optimizer = dict()\n",
    "optimizer['class'] = optim.Adam\n",
    "optimizer['params'] = dict(lr=.00025)\n",
    "\n",
    "# Settings\n",
    "\n",
    "# for the input of the network, we will used (history_length, height, width)\n",
    "width = 84\n",
    "height = 84\n",
    "history_length = 4 \n",
    "\n",
    "train_frequency = 4 # after 4 epochs train the network\n",
    "evaluation_frequency = 250000 # after 250000 epoch evaluate the agent\n",
    "target_update_frequency = 10000 #\n",
    "initial_replay_size = 50000\n",
    "max_replay_size = 500000\n",
    "test_samples = 125000\n",
    "max_steps = 50000000\n",
    "\n",
    "# MDP\n",
    "mdp = Atari('BreakoutDeterministic-v4', width, height, ends_at_life=True,\n",
    "            history_length=history_length, max_no_op_actions=30)\n",
    "\n",
    "# ends_at_life  -> whether the episode ends when a life is lost or not;\n",
    "# max_no_op_actions -> maximum number of no-op action to execute at the beginning of an episode.\n",
    "\n",
    "# Policy\n",
    "# LinearParameter -> This class implements a linearly changing parameter according to the number of times it has been used.\n",
    "epsilon = LinearParameter(value=1.,\n",
    "                          threshold_value=.1,\n",
    "                          n=1000000)\n",
    "epsilon_test = Parameter(value=.05)\n",
    "epsilon_random = Parameter(value=1)\n",
    "pi = EpsGreedy(epsilon=epsilon_random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb9f804b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approximator\n",
    "input_shape = (history_length, height, width)\n",
    "approximator_params = dict(\n",
    "    network=Network,\n",
    "    input_shape=input_shape,\n",
    "    output_shape=(mdp.info.action_space.n,),\n",
    "    n_actions=mdp.info.action_space.n,\n",
    "    n_features=Network.n_features, # the intermediate hidden layer 512\n",
    "    optimizer=optimizer,\n",
    "    loss=F.smooth_l1_loss # it is the absolute function loss # TODO: review it\n",
    ")\n",
    "\n",
    "approximator = TorchApproximator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01d5c082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent\n",
    "algorithm_params = dict(\n",
    "    batch_size=32,\n",
    "    target_update_frequency=target_update_frequency // train_frequency, # porque divido?\n",
    "    replay_memory=None,\n",
    "    initial_replay_size=initial_replay_size,\n",
    "    max_replay_size=max_replay_size\n",
    ")\n",
    "\n",
    "agent = DQN(mdp.info, pi, approximator,\n",
    "            approximator_params=approximator_params,\n",
    "            **algorithm_params)\n",
    "\n",
    "# Algorithm\n",
    "core = Core(agent, mdp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a87ec659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN\n",
    "\n",
    "# Fill replay memory with random dataset\n",
    "# learn: This function moves the agent in the environment and fits the policy using the collected samples.\n",
    "\n",
    "print_epoch(0)\n",
    "core.learn(n_steps=initial_replay_size,\n",
    "           n_steps_per_fit=initial_replay_size)\n",
    "\n",
    "# Evaluate initial policy\n",
    "pi.set_epsilon(epsilon_test)\n",
    "mdp.set_episode_end(False) # configura el mdp a un estado normal con todas las vidas\n",
    "dataset = core.evaluate(n_steps=test_samples) # evalua n_steps\n",
    "scores.append(get_stats(dataset)) # obtiene sus stats\n",
    "\n",
    "# cada epoca consiste de evaluation frequency steps\n",
    "for n_epoch in range(1, max_steps // evaluation_frequency + 1):\n",
    "    print_epoch(n_epoch)\n",
    "    print('- Learning:')\n",
    "    # learning step\n",
    "    pi.set_epsilon(epsilon)\n",
    "    mdp.set_episode_end(True) # vuelve el mdp a un modo que acaba cuando pierde la vida\n",
    "\n",
    "    # realizo n_steps en total pero realizo el fit the la red cada n_steps_per_fit\n",
    "    core.learn(n_steps=evaluation_frequency,\n",
    "               n_steps_per_fit=train_frequency)\n",
    "\n",
    "    print('- Evaluation:')\n",
    "    # evaluation step\n",
    "    pi.set_epsilon(epsilon_test)\n",
    "    mdp.set_episode_end(False)\n",
    "    dataset = core.evaluate(n_steps=test_samples)\n",
    "    scores.append(get_stats(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a782fcab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1fb548e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d315a9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
